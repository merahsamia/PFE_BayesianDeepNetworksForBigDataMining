\chapter*{Introduction générale}
Ces dernières années ont vu la convergence de trois révolutions : une révolution technologique, une révolution des données et une révolution des usages. Ensemble, ces trois révolutions constituent ce que l'on nomme «  Big Data ».\\[0.5\baselineskip]
Littéralement, ce terme signifie mégadonnées, grosses données ou encore données massives. Ils désignent un ensemble très volumineux de données qu'aucun outil classique de gestion de base de données ou de gestion de l'information ne peut vraiment travailler. En effet, nous procréons environ 2,5 trillions d'octets de données tous les jours. Ce sont les informations provenant de partout : messages que nous nous envoyons, vidéos que nous publions, informations climatiques, signaux GPS, enregistrements transactionnels d'achats en ligne et bien d'autres encore. Ces données sont baptisées Big Data ou volumes massifs de données. Les géants du Web, au premier rang desquels Yahoo (mais aussi Facebook et Google), ont été les tous premiers à déployer ce type de technologie.\\[0.5\baselineskip]
Ce concept regroupe une famille d'outils qui répondent à une triple problématique dite règle des 3V. Il s'agit notamment d'un \textbf{Volume} de données considérable à traiter, une grande \textbf{Variété} d'informations (venant de diverses sources, non-structurées, organisées, Open...), et un certain niveau de \textbf{Vélocité} à atteindre, autrement dit de fréquence de création, collecte et partage de ces données.\\[0.5\baselineskip]
Ces trois facteurs sont une composante essentielle du Big Data. Il faut nécessairement les considérer pour gérer, analyser et traiter la masse considérable d'informations circulant chaque jour. Le Big Data se présente comme une évolution à laquelle personne ne peut se soustraire.\\[0.5\baselineskip]
La quantité des données produite augmente constamment. En raison de leur quantité et de leur volume, les outils classiques de gestion et d'analyse sont   incapables de traiter convenablement ces données, ce qui rend leur traitement de plus en plus difficile à gérer avec les outils actuels.\\[0.5\baselineskip]
C'est là que l'intelligence artificielle intervient. Seuls des algorithmes sophistiqués de data mining sont aujourd'hui capables de traiter autant d'informations en instantanée. \\[0.5\baselineskip]
L'intelligence artificielle va être utilisée pour extraire du sens, déterminer de meilleurs résultats, et permettre des prises de décisions plus rapides à partir de sources Big Data massives.\\[0.5\baselineskip]
Aujourd'hui, l'usage des techniques de l'intelligence artificielle (machine learning, deep learning), des systèmes experts et des technologies analytiques en combinaison avec le Big Data se présentent comme l'évolution naturelle de ces deux disciplines. La convergence est inéluctable.\\[0.5\baselineskip]
Comme le dit le vieil adage \textbf{«  trop d'informations tuent l'information »}. Il s'agit en fait du principal problème avec les mégadonnées. La quantité énorme des informations est un des obstacles. L'autre obstacle provient évidemment du niveau de certitude qu'on peut avoir sur une donnée.\\[0.5\baselineskip]
Les outils d'apprentissage profond (deep learning) ont acquis une attention considérable dans l'apprentissage machine appliqué. Toutefois, ces outils ne tiennent pas compte de l'incertitude du modèle. En comparaison, les modèles bayésiens offrent un cadre mathématiquement fondé à la raison de l'incertitude du modèle.\\ [0.5\baselineskip]
A ce propos, on peut distinguer deux types d'écoles dans le domaine prédictif à savoir le deep learning et les réseaux bayésiens. Ces deux secteurs bien qu'ils soient distincts se rejoignent finalement de plus en plus. De plus, ils peuvent être utilisés en simultanéité de manière vertueuse et intelligente pour mener à bien un projet, c'est ce que l'on nomme \textbf{Les réseaux Bayésiens Profonds} (Bayesian Deep Networks en anglais).\\[0.5\baselineskip]
L'objectif de notre projet est d'implémenter les réseaux bayésiens profonds en proposant une approche adéquate. Pour cela, nous utilisons les réseaux de neurones convolutifs en ajoutant l'aspect bayésien dans le but de classifier un jeu de données d'images.\\[0.5\baselineskip]
Ce mémoire comporte trois chapitres, définis comme suit:
\begin{itemize}
	\item \textbf{Chapitre 1 « L'analyse du big data »:} 
	ce chapitre porte sur les outils d'analyse du big data, il offre une vision sur l'apprentissage automatique en se focalisant sur l'apprentissage profond et ses notions importantes et donne un petit aperçu sur les réseaux bayésiens.
	\item \textbf{Chapitre 2 « Réseaux bayésiens profonds »:} 
	ce second chapitre sera consacré aux réseaux bayésiens profonds ainsi qu'à l'implémentation de l'approche de Monte Carlo Dropout dans l'apprentissage profond bayésien en vue de prouver son efficacité pour explorer les valeurs du Big Data
	\item \textbf{Chapitre 3 « Implémentation et résultats expérimentaux »:} 
	ce dernier chapitre expose les résultats des différentes expérimentations réalisées.\\[0.01\baselineskip]
\end{itemize}
\newpage
Enfin, avant de clôturer ce mémoire en parlant des perspectives possibles, nous dressons une conclusion générale du projet.
 