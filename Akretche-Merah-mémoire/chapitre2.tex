\chapter{\sc Les réseaux bayésiens profonds }\label{chap2} 
\section{Introduction}
Nous introduisons dans ce second chapitre la notion des réseaux bayésiens profonds, nous citons quelques travaux associés, nous effectuons par la suite une description de l'état de l'art des approches utilisées pour l'implémentation des réseaux bayésiens profonds et plus particulièrement l'approche de Monte Carlo Dropout.
\section{Les réseaux bayésiens profonds }
La principale motivation derrière la prise en compte des réseaux bayésiens profonds (Deep Bayesian Networks en anglais) est la nécessité de fournir une modélisation simplifiée et précise des données de grande dimension. Les défis des Big Data nécessitent un modèle robuste qui est appris à partir des données volumineuses capturées dans un petit intervalle de temps. \\[0.5\baselineskip]
Il existe deux approches majeures pour l'apprentissage des réseaux bayésiens profonds. La première approche c'est d'apprendre une structure du réseau bayésien profond en proposant ses propres algorithmes de clustering \cite{ref18}. La deuxième approche est d'ajouter l'aspect bayésien aux architectures existantes du deep learning que nous appelons \textbf{l'apprentissage profond bayésien} \cite{ref34}.
\section{Travaux associés}
\subsection{Architecture du réseau bayésien profond pour le Big data mining \cite{ref18}}
Dans cet article les auteurs se sont inspirés du principe de l'apprentissage et des réseaux bayésiens hiérarchiques afin de fournir une nouvelle architecture de réseau bayésien multi-couches avec des variables latentes. Cette architecture est appelée  Deep Bayesian Network (Deep-BN). Il s'agit en effet d'un réseau bayésien arborescent où toutes les variables sont réparties en un nombre fini de couches; la couche la plus basique est composée de toutes les caractéristiques observées. Ils ont proposé également une méthode triviale et nouvelle pour l'apprentissage de cette architecture. Ils présentent une étape de regroupement de caractéristiques qui vise à trouver les groupes de variables très dépendantes et une étape d'apprentissage de variables latentes qui trouve la distribution de probabilité des nouvelles variables cachées abstraites. Le nombre d'itérations de ces étapes, c'est-à-dire le nombre de couches cachées, dépend de la quantité de perte d'information entre deux couches successives. Par conséquent, leur architecture a l'avantage d'être simplement apprise et facilement interprétée en raison de l'aspect graphique du réseau bayésien. Celui ci permet de réduire la dimension des fonctions dans un contexte de méga données tout en conservant le plus d'information possible. Il offre également un rendement amélioré en matière de classification.
\subsection{Réseau de neurones bayésien à convolution profonde pour l'analyse de la qualité des SMS \cite{ref27}}
Pour obtenir une communication pratique et économique, le short message service (SMS) est l'un des moyens les plus faciles et les plus réalisables. Cependant, de plus en plus de messages indésirables tels que la publicité constituent un  plus grand défi pour ce service. Dans cet article, les auteurs ont proposé une nouvelle architecture de réseau de neurones convolutifs "Convolutional neural network (CNN)" basée sur l'apprentissage bayésien pour réaliser une évaluation de la qualité des SMS. Plus précisément, ils forment d'abord le vecteur de mot de chaque mot dans l'ensemble de données du SMS, et convertissent le groupe de vecteurs de mots de chaque SMS en une matrice bidimensionnelle. Par la suite, la matrice de caractéristiques est utilisée comme entrée dans le réseau de neurones convolutifs, dans lequel les caractéristiques du SMS sont extraites par les noyaux de convolution, et l'extraction des caractéristiques est simplifiée par l'algorithme bayésien. Enfin, le score de qualité peut être atteint en fonction des caractéristiques optimales locales.
\newpage
\subsection{Construction des réseaux de neurones profonds par l'apprentissage de la structure d'un réseau bayésien \cite{ref33}}
Il existe deux approches qui ont été étudiées pour l'apprentissage de la structure des modèles graphiques probabilistes. Plus précisément, les réseaux bayésiens pour l'estimation de la densité et la découverte des causes: L'une basée sur les scores et l'autre sur les contraintes. Motivés par les deux méthodes d'apprentissage de la structure des modèles graphiques probabilistes. Ces auteurs ont proposé une interprétation de la profondeur et de la connectivité inter-couches dans les réseaux de neurones profonds pour en extraire un Algorithme d'apprentissage de la structure de telle sorte qu'une hiérarchie des indépendances dans la distribution d'entrées est encodée dans un graphe génératif profond, où les indépendances d'ordre inférieur sont encodées dans des couches plus profondes. Ainsi, le nombre de couches est automatiquement déterminé, ce qui est souhaitable dans toute méthode d'apprentissage de l'architecture.\\[0.5\baselineskip]
Les auteurs commencent par convertir le graphe génératif en un graphe discriminant, démontrant la capacité de ce dernier à imiter (préserver les dépendances conditionnelles) du premier. Dans la structure résultante, un neurone dans une couche est autorisé à se connecter aux neurones dans des couches plus profondes en sautant des couches intermédiaires. En outre, les neurones des couches plus profondes représentent des indépendances d'ordre faible (petits ensembles de conditions) et ont une large portée d'entrée, tandis que les neurones des premières couches représentent des indépendances d'ordre supérieur (ensembles de conditions plus grands) et ont une portée plus étroite.
\subsection{Reconnaissance faciale avec les réseaux convolutifs bayésiens pour des systèmes de surveillance robustes \cite{ref34}}
La reconnaissance abstraite des images faciales est l'un des problèmes de recherche les plus difficiles dans les systèmes de surveillance en raison de différents problèmes, notamment la pose, l'expression, l'éclairage et la résolution. Le processus de reconnaissance faciale consiste à identifier la personne en comparant certaines caractéristiques d'une nouvelle personne (échantillon d'entrée) avec les personnes connues dans la base de données. La robustesse de la méthode de reconnaissance repose fortement sur la force des caractéristiques extraites et la capacité de traiter des images de visage de faible qualité. La capacité d'apprendre des caractéristiques robustes à partir d'images brutes du visage rend les réseaux neuronaux convolutionnels profonds (Dcnns) attrayants pour la reconnaissance du visage.\\[0.5\baselineskip]
\cite{ref34} ont utilisé la méthode dropout (le décrochage) pour former une architecture DCNN bayésien (B-DCNN). Il est montré qu'il y'a une relation entre le dropout et l'inférence variationnelle dans les B-DCNNs avec les distributions de Bernoulli sur les poids du réseau. Dans ce travail, les auteurs ont utilisé cette approche pour représenter les incertitudes du modèle tout en classant les images faciales. Le décrochage est également utilisé au moment du test pour échantillonner la distribution postérieure sur les poids. La moyenne et la variance des échantillons sont utilisées respectivement comme confiance et incertitude pour chaque classe. La décision finale de classification est prise sur la base d'une fonction heuristique simple.
\subsection{Réseaux de neurones bayésiens convolutifs avec inférence variationnelle approximative de Bernoulli \cite{ref35}}
Les réseaux de neurones convolutifs (CNNs) fonctionnent bien sur de grands ensembles de données. Mais les données étiquetées sont difficiles à collecter. Donc le problème qui se pose est alors de savoir comment utiliser les CNNs avec de petits ensembles de données - car les CNNs tombent dans le surapprentissage rapidement. Dans cet article, les auteurs présentent un CNN bayésien efficace, offrant une résistance face au surapprentissage sur des petits ensembles de données contrairement aux anciennes approches. Pour ce faire, ils commencent par placer une distribution de probabilité sur les noyaux du CNN. Ils présentent de nouveaux résultats théoriques utilisant le dropout lors de l'apprentissage des réseaux autant qu'une inférence approximative dans les réseaux de neurones bayésiens. Cela a permet de réaliser le modèle employé dans cet \oe{}uvre en utilisant les outils existants sur le terrain sans augmentation de la complexité temporelle. Ils approximent la partie postérieure intraitable du modèle avec les distributions variationnelle de Bernoulli, sans avoir besoin de paramètres supplémentaires. Les résultats montrent une amélioration considérable de la précision de la classification par rapport aux techniques standards utilisées sur le CIFAR10.
\subsection{Bayesian SegNet : Incertitude du modèle dans les architectures d'encodeurs-décodeurs convolutifs profonds pour la compréhension des scènes \cite{ref36}}
Dans cet article, ils présentent un cadre d'apprentissage profond pour la segmentation sémantique probabiliste en pixels, qu'ils appellent Bayesian SegNet. La segmentation sémantique est un outil important pour la compréhension de la scène visuelle et une mesure significative de l'incertitude est essentielle pour la prise de décision. Cette contribution est un système pratique qui est capable de prédire les étiquettes de classe pixel par pixel avec une mesure de l'incertitude du modèle en utilisant l'apprentissage bayésien profond. Ils parviennent par un échantillonnage Monte Carlo avec le dropout au moment du test pour générer une distribution postérieure des étiquettes des classes de pixels. Ils montrent aussi que l'incertitude de la modélisation améliore les performances de segmentation de 2 à 3 \% pour un certain nombre d'ensembles de données et d'architectures tels que SegNet, FCN, Dilation Network et DenseNet.
\subsection{Le dropout comme approximation bayésienne : Représentation de l'incertitude du modèle dans l'apprentissage profond \cite{ref37}}
Les outils d'apprentissage profond ont acquis une attention considérable dans les applications de l'apprentissage machine. Toutefois, ces outils de régression et de classification ne tiennent pas compte de l'incertitude du modèle. En comparaison, les modèles bayésiens offrent un cadre mathématiquement pour estimer l'incertitude du modèle, mais viennent généralement avec un coût de calcul énorme. Dans cet article, les auteurs développent un nouveau cadre théorique qui permet d'utiliser le dropout dans les réseaux de neurones profonds (NNs) comme inférence bayésienne approximative dans les processus gaussiens profonds. Un résultat direct de cette théorie, donne des outils pour modéliser l'incertitude avec les réseaux de neurones profonds en extrayant des informations des modèles existants qui ont été négligés jusqu'à présent. Cela permet de réduire le problème de la représentation de l'incertitude dans l'apprentissage profond sans sacrifier ni la complexité du calcul ni la précision des tests. Ils réalisent dans cet ?uvre, une étude approfondie des propriétés de l'incertitude du Dropout. Diverses architectures de réseau sont évaluées sur des tâches de régression et de classification, en utilisant le MNIST comme exemple. Ils approuvent une amélioration considérable de la fonction prédictive du log-vraisemblance ainsi que sur l'erreur quadratique moyenne par rapport aux méthodes existantes.
\subsection{Génération de conversation émotionnelle basée sur un réseau neuronal bayésien profond \cite{ref38}}
Le domaine de la génération de conversations à l'aide de réseaux neuronaux attire de plus en plus l'attention des chercheurs depuis plusieurs années. Cependant, les modèles de langage neuronal traditionnels ont tendance à générer une réponse générique avec une mauvaise logique sémantique et aucune émotion. Cet article propose un modèle de génération de conversation émotionnelle basé sur un réseau neuronal bayésien profond qui peut générer des réponses avec des émotions riches, des thèmes clairs et des phrases diverses. Le sujet et les mots-clés émotionnels des réponses sont pré-générés en introduisant la connaissance du bon sens dans le modèle. La réponse est divisée en plusieurs clauses, puis un générateur multidimensionnel basé sur le mécanisme transformateur proposé dans cet article est utilisé pour générer itérativement des clauses à partir de deux dimensions : la granularité des phrases et la structure des phrases. Les expériences subjectives et objectives prouvent que, par rapport aux modèles existants, le modèle proposé améliore efficacement la logique sémantique et la précision émotionnelle des réponses. Ce modèle améliore également considérablement la diversité des réponses, en surmontant largement les lacunes des modèles traditionnels qui génèrent des réponses sûres.
\section{L'apprentissage profond bayésien}
Le DEEP Learning a obtenu un succès significatif dans de nombreuses tâches de perception, y compris la reconnaissance visuelle des objets, la compréhension de  textes et la reconnaissance vocale. Il s'agit sans aucun doute de tâches fondamentales pour un système complet d'intelligence artificielle (IA) ou d'ingénierie des données (DE) fonctionnel. Cependant, pour construire un véritable système d'IA/DE, le simple fait de pouvoir voir, lire et entendre est loin d'être suffisant. Il devrait surtout avoir la capacité de penser.\\[0.5\baselineskip]
Dans presque tous les problèmes du monde réel, ce que nous voulons, ce n'est pas seulement un résultat, mais nous avons aussi besoin d'une connaissance de la confiance / certitude dans ce résultat. Lorsque l'apprentissage profond est utilisé dans des domaines sensibles tels que la santé et le contrôle autonome, nous devrions nous interroger non seulement sur la précision mais aussi sur la confiance des modèles déployés.\\[0.5\baselineskip]
Prenons l'exemple du diagnostic médical. En plus de voir des symptômes visibles (ou des images médicales) et d'entendre des descriptions de patients, un médecin doit rechercher des relations entre tous les symptômes et de préférence inférer l'étiologie correspondante. Seulement après cela le médecin peut fournir des conseils médicaux pour les patients. Dans cet exemple, bien que les capacités de voir et d'entendre permettent au médecin d'acquérir de l'information auprès des patients, c'est la partie de la pensée qui définit un médecin. En particulier, la capacité de penser ici pourrait impliquer l'inférence causale, la déduction logique et le traitement de l'incertitude, ce qui est apparemment au-delà de la capacité des méthodes conventionnelles d'apprentissage en profondeur.\\[0.5\baselineskip]
Un autre exemple, nous aimerions qu'un véhicule autonome non seulement identifie correctement les objets dans son environnement, mais fournisse également une mesure de l'incertitude. De même, si nous écrivons un bot qui négocie sur le marché de la bourse, nous voulons qu'il reconnaisse, quand la situation sort de sa zone de confort, de sorte qu'il peut arrêter d'agir et ne pas faire faillite.\\[0.5\baselineskip]
Une grande partie de l'intelligence n'agit pas quand on est incertain. Il est donc surprenant que pour de nombreux projets de ML, exprimer l'incertitude n'est pas ce qui est visé.\\[0.5\baselineskip]
Les outils standards d'apprentissage profond pour la régression et la classification ne saisissent pas l'incertitude du modèle. Dans la classification, les probabilités prédictives obtenues à la fin (la sortie softmax) sont souvent interprétées à tort comme une confiance du modèle (si c'est 0,8, alors mon modèle est sûr à 80 \% de sa prédiction). Yarin Gal \cite{web10} argumente, qu'un modèle peut être incertain dans ses prédictions même avec une sortie softmax élevée.\\[0.5\baselineskip]
Heureusement, un autre type de modèles existe, les modèles bayésiens, qui excellent dans l'inférence et la gestion de l'incertitude. Le problème est que le modèle bayésien n'est pas aussi bon que les modèles d'apprentissage profond pour les tâches de perception. Pour résoudre le problème, il est donc naturel d'intégrer étroitement l'apprentissage profond et le modèle bayésien dans un cadre probabiliste fondé sur des principes, que nous appelons l'apprentissage profond bayésien (BDL).\\[0.5\baselineskip]
L'idée principale de l'apprentissage profond bayésien est d'introduire la notion de probabilité durant la phase d'entraînement et la phase de prédiction du réseau. Pour ce faire, les instances manipulées sont des distributions de probabilités et non des scalaires. Ainsi, les poids du réseau et les valeurs contenues des neurones suivent des lois normales qui sont caractérisées par une valeur moyenne et un écart-type.
Ceci constitue la seule différence conceptuelle notable entre l'apprentissage profond et l'apprentissage profond bayésien (les autres différences étant d'ordre mathématique et algorithmique). Finalement, chaque neurone de la couche de sortie du réseau retourne une loi normale dont la valeur moyenne et l'écart type ont une valeur déterminée par l'image d'entrée du réseau. De ces distributions, il est possible à l'aide de formules mathématiques de calculer les différentes incertitudes \cite{web11}.\\[0.5\baselineskip]
Plusieurs formulations bayésiennes récentes du deep learning fournissent des techniques alternatives pour extraire des estimations d'incertitude à partir des réseaux profonds, y compris l'approche de Monte Carlo Dropout \cite{ref37} (qui sera développée par la suite), précédemment employée dans l'apprentissage actif profond pour la classification des images \cite{ref39} et la reconnaissance des entités désignées \cite{ref40} et l'approche de Bayes-by-Backprop.\cite{ref41}.
\section{Monte Carlo Dropout comme approximation bayésienne}
\subsection{Dropout}
C'est une technique de régularisation (pour combattre l'overfitting) dont le principe est de désactiver aléatoirement à chaque itération un certain pourcentage des neurones d'une couche. Cela évite ainsi la sur-spécialisation d'un neurone (et donc l'apprentissage par c\oe ur) \cite{ref42}. Il faut que la probabilité qu'un neurone ne soit pas désactivé soit entre 0.5 et 0.8 pour obtenir les meilleurs résultats \cite{ref43}. Ceci peut se faire par un tirage aléatoire. Par exemple, en jetant une pièce pour décider de la désactivation d'un neurone, on obtiendrait une probabilité de 0.5. Le nombre de neurones est donc réduit mais de façon aléatoire à chaque itération d'apprentissage. Cette technique fonctionne bien dans la généralisation d'un réseau.
\newpage
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{./image_chapitre1/figure_11}
	\caption[Un réseau de neurones lors de l'application de la technique de Dropout]{Un réseau de neurones lors de l'application de la technique de Dropout \cite{ref43}}
	\label{fig:figure_11}
\end{figure}
Cette technique n'est utilisée que pendant la phase d'apprentissage. Le réseau neuronal se voit amputé d'une partie de ses neurones pendant la phase d'entrainement (leur valeur est estimée à 0) et ils sont par contre réactivés pendant la phase de test.
\subsection{Estimation d'incertitude}
La notion d'incertitude en apprentissage automatique se divise en deux catégories: les incertitudes épistémiques et les incertitudes aléatoires \cite{ref44}. Bien que cette distinction entre incertitudes aléatoires et épistémiques semble abstraite au premier abord, elle représente, en réalité, des notions intuitives.
\begin{itemize}
	\item Les incertitudes aléatoires sont associées à la qualité des données. Dans le cadre d'une base d'images, cela correspondrait à des incertitudes dûes au niveau de flou ou au manque de netteté de l'image.
	\item Les incertitudes épistémiques correspondent aux incertitudes dûes à la qualité du modèle. Il s'agit d'une incertitude liée au fait de ne pas connaître les meilleures valeurs de poids dans toutes les couches.
\end{itemize}
Ce sont ces dernières qui apportent le plus d'informations sur la validité de la prédiction du modèle \cite{ref45}.
\subsubsection{L'incertitude épistémique dans l'apprentissage profond bayésien \cite{ref45}}
Compte tenu de l'ensemble de données $ D(X,Y) $, où $ X={x_1,...,x_N} $ contient les enregistrements de toutes les variables prédictives et $ Y={y_1,...,y_N} $ porte la variable de résultat, nous aimerions prédire de nouveaux $ y^* $ étant donné un nouveau point de données $ w^* $, avec un certain modèle de réseaux de neurones défini par l'ensemble de poids de couche ou de paramètres $ W $. Nous formulons généralement la tâche de trouver le $ W $ optimal comme problème d'optimisation et on le résout en utilisant des techniques telles que la descente de gradient stochastique. En conséquence, nous obtenons un seul ensemble $ W $ et donc une seule prédiction de $ y^* $.\\[0.5\baselineskip]
Par contre, dans les réseaux de neurones bayésiens on essaie de modéliser la distribution de W, et par la suite la distribution prédictive postérieure de $ y^* $.\\
\begin{eqnarray}
	P(D) = \int P(D|W)P(W)d(W)
\end{eqnarray}
\begin{eqnarray}
	P(W|D) = \frac{P(D)P(W)}{P(D)}
\end{eqnarray}
\begin{eqnarray}
	P(y^*|x^*,D)= \int P(y^*|x^*,W)P(W|D)dW
\end{eqnarray}\\[0.5\baselineskip]
Cependant, le problème ici est que le calcul de l'intégrale pour P(D) est faisable pour très peu de cas de réseaux de neurones or que ce n'est pas le cas pour le calcul de l'intégrale de la distribution postérieure de $ y $.\\[0.5\baselineskip]
Pour traiter des cas comme celui-ci où l'inférence exacte est difficile, les bayésiens ont développé deux familles majeures d'approches pour faire l'inférence approximative. L'une est l'inférence variationnelle (VI) et l'autre est Markov Chain Monte Carlo (MCMC). On s'intéresse dans ce travail à l'inférence variationnelle et plus particulièrement à l'une de ses variantes qui est le MC Dropout.
\subsection{L'inférence variationnelle (VI) \cite{ref45}}
La première idée clé dans l'inférence variationnelle est de remplacer  $ P(W|D) $ dans l'équation (2.3) par une distribution variationnelle approximative $ q_\theta (W_i)  $, qui est paramétrée par $ \theta $ et peut être évaluée.\\
\begin{eqnarray}
	q_\theta^* = \int P(y^*|x^*,W)q_\theta(W)d(W) \approx P(y^*|x^*,D)
\end{eqnarray}\\
Nous voudrions que ce $ q_\theta (W_i)  $ ressemble le plus possible à $ P(W|D) $. Idéalement, nous le ferions en minimisant la divergence Kullbeck-Leibler (KL) entre les deux :\\
\begin{eqnarray}
	KL(q_\theta(W) | P(W|D)) = \int q_\theta(W)\log\frac{q_\theta(W)}{P(D|W)}dW
\end{eqnarray}\\ 
Nous ne pouvons pas minimiser l'équation (2.5) car elle contient $ P(W|D) $. Cependant, il a été constaté que nous pouvons réduire au minimum l'équation (2.5) en maximisant la limite inférieure des données probantes (ELBO).\\
\begin{eqnarray}
	\psi_{v1}(\theta) = \int q_\theta(W)\log P(D|W)dW - KL(q_\theta(W) | P(W)) \leq \log P(D)
\end{eqnarray}\\
Il faut noter que le premier terme de l'équation (2.6) représente le log-vraisemblance conditionnelle.
\subsection{L'approximation de Monte-Carlo Dropout}
Cette méthode récemment inventée par \cite{ref37} s'appelle le \textbf{Monte-Carlo Dropout}. Il est intéressant d'expliciter les termes composant le nom de cette méthode : Monte-Carlo fait référence à une famille d'algorithmes effectuant des prédictions grâce à des processus aléatoires et le Dropout mentionné précédemment.\\[0.5\baselineskip]
L'idée de cette technique est d'utiliser le dropout durant et la phase d'entrainement et la phase de test pour que par exemple à partir  d'une seule entrée on puisse avoir plusieurs valeurs en sortie différentes à travers T MCD forward passes (quelques centaines de fois), c.à.d. passer la même entrée au réseau et par la suite appliquer un dropout aléatoire. Intuitivement, le modèle est capable de donner des prédictions différentes puisque différentes combinaisons de neurones sont utilisées pour la prédiction. En outre, cette méthode est en effet bayésienne.\\[0.5\baselineskip]
De ces valeurs de sortie différentes, on peut obtenir une valeur moyenne et un écart-type par neurone de sortie. On obtient donc des résultats similaires aux résultats obtenus par un réseau bayésien ordinaire mais avec un réseau qui a été entraîné de manière classique \cite{web11} \cite{web12}.\\[0.5\baselineskip]
Dans MC dropout, nous définissons $ q_\theta(W_i) $, la distribution approximative pour les poids de chaque couche i :\\
\begin{eqnarray}
	W_i =M_i. diag([Z_{i,j}]_{j=0}^{ki})
\end{eqnarray}
\begin{eqnarray}
	Z_{i,j} \sim Bernoulli(p_i) pour i= 1,...,L ,j = 1,...,K_{i-1}
\end{eqnarray}\\
Où chaque $ Z_{i,j} $  est une variable aléatoire de Bernoulli décidant si l'entrée connectée doit être abandonnée ou non, avec une probabilité $ p_i $. Par conséquent, $ p_i $ représente la probabilité de conserver l'entrée, ce qui est exactement le contraire de ce que nous appelons le taux de dropout \cite{ref45}.\\[0.5\baselineskip]
On souhaite calculer l'équation (2.6), mais il n'est pas possible d'évaluer directement la première partie avec intégrale. \cite{ref37} et \cite{ref46} proposent une solution à ce propos qui peut se résumer en deux parties :
\begin{enumerate}
	\item Nous pourrions rapprocher cette intégrale et obtenir un estimateur sans biais pour $ \psi_{v1}(\theta) $, en minimisant les pertes typiques avec la régularisation L2, typiquement utilisé dans l'apprentissage des modèles de réseaux de neurones et prend la forme de :\\
\begin{eqnarray}
	\psi_{dropout} = \frac{1}{N}\sum_{i=1}^{N}E(y_i, y^{'}_i) + \lambda\sum_{i=1}^{L}(||W_i||_2^2+||b_i||_2^2)
\end{eqnarray}\\
Où $ E(y_i,y^{'}_i) $ désigne des fonctions de perte telles que softmax ou l'erreur       quadratique moyenne.
\item Cette inférence variationnelle est équivalente à une approximation par processus Gaussien (GP) du réseau neuronal avec une précision de modèle $ \tau $ et une échelle de longueur l, ce qui signifie que nous obtenons une approximation de la distribution sur les fonctions possibles compte tenu des points de données.\\[0.5\baselineskip]
Nous pouvons maintenant obtenir un postérieur prédictif approximatif défini dans l'équation (2.3), ainsi que sa moyenne et sa variance:\\
\begin{eqnarray}
	P(y^*|x^*,D) = N(y^*; f^W(x^*), \tau^{-1}I)
\end{eqnarray}
\begin{eqnarray}
	E_{q\theta(y^*|x^*)}(y^*) \equiv \frac{1}{T}\sum_{t=1}^{T}f^W(x^*)
\end{eqnarray}
\begin{eqnarray}
	\begin{split} 
	Var_{q\theta(y^*|x)}(y^*) \equiv \tau^{-1}I_D + \frac{1}{T}\sum_{t=1}^{T}(f^W(x^*))^Tf^W(x^*) - \\
	(E_{q\theta(y^*|x^*)}(y^*))^TE_{q\theta(y^*|x^*)}(y^*)
    \end{split} 
\end{eqnarray}\\
Où nous établissons la moyenne des T tests du réseau formé à l'équation (2.9).\\[0.5\baselineskip]
Il convient de noter que, bien que MC Dropout vise à saisir l'incertitude du modèle, sa formulation n'élimine pas complètement l'incertitude aléatoire. Au contraire, comme le montre l'équation (2.10), elle suppose une incertitude aléatoire, représentée par $ \tau^{-1}I $, indiquant le bruit de données homogènes \cite{ref45}.
\end{enumerate}
\newpage
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{./image_chapitre1/figure_12}
	\caption[Fonctionnement de dropout \cite{web13}]{Fonctionnement de dropout \cite{web13}}
	\label{fig:figure_12}
\end{figure}
Ici, nous pouvons voir comment le dropout crée un ensemble de réseaux, qui donnent chacun une sortie différente, et cela nous permet d'avoir une distribution sur la sortie de notre réseau. De cette façon, nous pouvons voir qu'il y a des mesures d'incertitude, comme la variance pour la régression ou l'entropie pour la classification.\\[0.5\baselineskip]
Les avantages de cette méthode sont multiples en cela que l'entraînement du réseau ne nécessite aucun aménagement et peut être réalisé par le framework d'apprentissage profond de choix (Keras, Tensorflow, Pytorch, etc.). De plus, il est possible, en adaptant légèrement un réseau de neurones classique déjà entraîné, d'obtenir son approximation bayésienne, il suffit juste d'ajouter des couches de dropout après chaque couche avec des paramètres de poids, et de faire T prédictions de test. Le seul inconvénient de cette méthode est le temps de calcul lors de la prédiction \cite{web11}.
\section{Conclusion}
Dans ce chapitre, nous avons présenté des généralités sur les réseaux bayésiens profonds, nous avons aussi expliqué le principe de l'approche de Monte Carlo Dropout. Une application de cette approche pour la classification dans un grand ensemble de données, fera l'objet du troisième chapitre.     